{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This script is responsible for intial results of \n",
    "cleaning CSV movie_metadata retrieved from kaggle.com\n",
    "'''\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "movie_df = pd.read_csv('movie_metadata.csv')\n",
    "\n",
    "# Deleting columns that are not useful \n",
    "delete = ['color', 'num_critic_for_reviews', 'actor_3_facebook_likes', 'actor_1_facebook_likes','movie_imdb_link', 'country', 'language', \n",
    "         'aspect_ratio', 'num_user_for_reviews', 'num_voted_users', 'actor_2_facebook_likes', 'content_rating', 'director_facebook_likes',\n",
    "         'cast_total_facebook_likes', 'movie_facebook_likes']\n",
    "for d in delete: del movie_df[d]\n",
    "\n",
    "# Delete rows with empty cells for any column     \n",
    "for c in movie_df: \n",
    "    movie_df[c].replace('', np.nan, inplace=True)\n",
    "    movie_df.dropna(subset=[c], inplace=True)\n",
    "\n",
    "movie_df = movie_df.reset_index(drop=True)\n",
    "\n",
    "genres_set = set() \n",
    "for i in range(len(movie_df)): \n",
    "    g = movie_df['genres'][i].split('|')\n",
    "    genres_set.update(g)\n",
    "\n",
    "genres = list(genres_set)\n",
    "genres = dict.fromkeys(genres)\n",
    "genres_ct = {key: 0 for key in genres} \n",
    "\n",
    "# Count number of movies that belong to each genre\n",
    "for i in range(len(movie_df)): \n",
    "    g = movie_df['genres'][i].split('|')\n",
    "    for item in g: \n",
    "        if item in genres: \n",
    "            genres_ct[item] += 1 \n",
    "\n",
    "# Get the top 5 genres \n",
    "sortedCounts = [(genres_ct[key], key) for key in genres_ct]\n",
    "sortedCounts.sort()\n",
    "sortedCounts.reverse() \n",
    "top_genres = [item[1] for item in sortedCounts[:5]] \n",
    "\n",
    "# Build list of row indices that need to dropped because they are not in the top 5 genres\n",
    "topg_set = set(top_genres) \n",
    "drop = [] \n",
    "for i in range(len(movie_df)):\n",
    "    g = set(movie_df['genres'][i].split('|'))\n",
    "    if len(set.intersection(g, topg_set)) == 0:\n",
    "        drop.append(i)\n",
    "\n",
    "movie_df = movie_df.drop(movie_df.index[drop])\n",
    "movie_df = movie_df.reset_index(drop=True) \n",
    "movie_df.to_csv('movie_clean_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This script is responsible for scraping \n",
    "additional movie data from imdb.com\n",
    "'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import csv\n",
    "import re\n",
    "import datetime\n",
    "import cognitive_face as CF\n",
    "import time\n",
    "\n",
    "# Create or overwrite 'movie_scrape.csv' file\n",
    "columns = ['director_name', 'duration', 'actor_2_name', 'gross', 'genres', 'actor_1_name', 'movie_title', 'actor_3_name', 'plot_keywords', 'budget', 'title_year', 'imdb_score']\n",
    "csv_file = open('movie_scrape_2006.csv', 'w')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(columns)\n",
    "\n",
    "# Helper function to remove all non-numeric characters from a string\n",
    "def removeNonNumeric(str):\n",
    "    return re.sub(\"[^0-9]\", \"\", str)\n",
    "\n",
    "# Function to scrape data from IMDB box office results\n",
    "def scrapeData(soup):\n",
    "    \n",
    "    # Finds the link for next page\n",
    "    pagination = soup.find('div', {'class':'desc'})\n",
    "    link_tag = pagination.find('a', {'class':'lister-page-next next-page'})\n",
    "    \n",
    "    for title in soup.find_all('h3', class_='lister-item-header'):\n",
    "        \n",
    "        movie_title = title.find('a').text\n",
    "        movie_url = title.find('a')['href']\n",
    "        m = urllib.request.urlopen('https://www.imdb.com/' + movie_url).read()\n",
    "        msoup = BeautifulSoup(m, 'html.parser')\n",
    "        \n",
    "        try:\n",
    "            imdb_score = msoup.find_all('span', attrs={'itemprop':'ratingValue'})[0].text\n",
    "        except IndexError:\n",
    "            imdb_score = ''\n",
    "            \n",
    "        try:\n",
    "            genre_div = msoup.find_all('div', itemprop='genre')[0].find_all('a')\n",
    "            i, genres = 0, ''\n",
    "            for g in genre_div:\n",
    "                i += 1\n",
    "                if i == len(genre_div):\n",
    "                    genres += g.text\n",
    "                else:\n",
    "                    genres += g.text + ' |'\n",
    "        except IndexError:\n",
    "            genres = ''\n",
    "\n",
    "        try:\n",
    "            director_name = msoup.find_all(attrs={'itemprop':'director'})[0].find('span').text\n",
    "        except IndexError:\n",
    "            director_name = ''\n",
    "        \n",
    "        try:\n",
    "            stars_spans = msoup.find_all(attrs={'itemprop':'actors'})\n",
    "            i, actor_1_name, actor_2_name, actor_3_name = 1, '', '', ''\n",
    "            for s in stars_spans:\n",
    "                if i == 1:\n",
    "                    actor_1_name += s.find('span').text\n",
    "                elif i == 2:\n",
    "                    actor_2_name += s.find('span').text\n",
    "                else:\n",
    "                    actor_3_name += s.find('span').text\n",
    "                i += 1\n",
    "        except TypeError:\n",
    "            actor_1_name, actor_2_name, actor_3_name = '', '', ''\n",
    "        \n",
    "        try:\n",
    "            details_divs = msoup.find_all('div', id='titleDetails')[0].find_all('div')\n",
    "            budget, gross, title_year, duration = '', '', '', ''\n",
    "            for d in details_divs:\n",
    "                try:\n",
    "                    if d.find('h4').text == 'Budget:':\n",
    "                        budget = removeNonNumeric(d.text.replace('\\n',''))\n",
    "                    elif d.find('h4').text == 'Gross:':\n",
    "                        gross = removeNonNumeric(d.text.replace('\\n',''))\n",
    "                    elif d.find('h4').text == 'Release Date:':\n",
    "                        date = d.text.split('\\n')[1].split('\\s')[0]\n",
    "                        title_year = re.search('\\d{4}', date).group(0)\n",
    "                    elif d.find('h4').text == 'Runtime:':\n",
    "                        duration = removeNonNumeric(d.text.replace('\\n',''))\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "        except IndexError:\n",
    "            budget, gross, title_year, duration = '', '', '', ''\n",
    "\n",
    "        try: \n",
    "            keywords_span = msoup.find_all('span', attrs={'itemprop':'keywords'})\n",
    "            keywords = [key.text for key in keywords_span]\n",
    "            i, plot_keywords = 0, ''\n",
    "            for word in keywords: \n",
    "                i += 1 \n",
    "                if i == len(keywords):\n",
    "                    plot_keywords += word \n",
    "                else: \n",
    "                    plot_keywords += word + '|'\n",
    "        except TypeError:\n",
    "            plot_keywords = ''\n",
    "        \n",
    "        new_entry = [director_name, duration, actor_2_name, gross, genres, actor_1_name, movie_title, actor_3_name, plot_keywords, budget, title_year, imdb_score]\n",
    "        csv_writer.writerow(new_entry)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "    return link_tag\n",
    "\n",
    "# url_base = 'http://www.imdb.com/search/title' \n",
    "# next_page = '?year=2015,2015&title_type=feature&sort=boxoffice_gross_us,desc'\n",
    "# stop_ct = 0\n",
    "# while True: \n",
    "#     r = urllib.request.urlopen(url_base + next_page).read()\n",
    "#     soup = BeautifulSoup(r, 'html.parser')\n",
    "#     tag = scrapeData(soup)\n",
    "#     stop_ct += 1\n",
    "#     print('Retrieving next page of movies')\n",
    "#     if tag is None or stop_ct <= 20: \n",
    "#         print('Done')\n",
    "#         break\n",
    "#     else:\n",
    "#         next_page = tag['href']\n",
    "\n",
    "# url_base = 'http://www.imdb.com/search/title' \n",
    "# next_page = '?year=2014,2014&title_type=feature&sort=boxoffice_gross_us,desc'\n",
    "# stop_ct = 0\n",
    "# while True: \n",
    "#     r = urllib.request.urlopen(url_base + next_page).read()\n",
    "#     soup = BeautifulSoup(r, 'html.parser')\n",
    "#     tag = scrapeData(soup)\n",
    "#     stop_ct += 1\n",
    "#     print('Retrieving next page of movies')\n",
    "#     if tag is None or stop_ct <= 20: \n",
    "#         print('Done')\n",
    "#         break\n",
    "#     else:\n",
    "#         next_page = tag['href']\n",
    "\n",
    "# url_base = 'http://www.imdb.com/search/title' \n",
    "# next_page = '?year=2013,2013&title_type=feature&sort=boxoffice_gross_us,desc'\n",
    "# stop_ct = 0\n",
    "# while True: \n",
    "#     r = urllib.request.urlopen(url_base + next_page).read()\n",
    "#     soup = BeautifulSoup(r, 'html.parser')\n",
    "#     tag = scrapeData(soup)\n",
    "#     stop_ct += 1\n",
    "#     print('Retrieving next page of movies')\n",
    "#     if tag is None or stop_ct <= 20: \n",
    "#         print('Done')\n",
    "#         break\n",
    "#     else:\n",
    "#         next_page = tag['href']\n",
    "\n",
    "# url_base = 'http://www.imdb.com/search/title' \n",
    "# next_page = '?year=2012,2012&title_type=feature&sort=boxoffice_gross_us,desc'\n",
    "# stop_ct = 0\n",
    "# while True: \n",
    "#     r = urllib.request.urlopen(url_base + next_page).read()\n",
    "#     soup = BeautifulSoup(r, 'html.parser')\n",
    "#     tag = scrapeData(soup)\n",
    "#     stop_ct += 1\n",
    "#     print('Retrieving next page of movies')\n",
    "#     if tag is None or stop_ct <= 20: \n",
    "#         print('Done')\n",
    "#         break\n",
    "#     else:\n",
    "#         next_page = tag['href']\n",
    "\n",
    "url_base = 'http://www.imdb.com/search/title' \n",
    "next_page = '?year=2006,2006&title_type=feature&sort=boxoffice_gross_us,desc'\n",
    "stop_ct = 0\n",
    "while True: \n",
    "    r = urllib.request.urlopen(url_base + next_page).read()\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "    tag = scrapeData(soup)\n",
    "    stop_ct += 1\n",
    "    print(stop_ct)\n",
    "    if stop_ct > 21: \n",
    "        print('Done')\n",
    "        break\n",
    "    else:\n",
    "        next_page = tag['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6082\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "'''\n",
    "This script is responsible for cleaning and merging the \n",
    "scraped movie data with the existing movie data set\n",
    "'''\n",
    "# Helper function to delete rows with empty cells for any column in movie dataframe    \n",
    "def deleteEmptyCol(movie_df):\n",
    "    for c in movie_df: \n",
    "        movie_df[c].replace('', np.nan, inplace=True)\n",
    "        movie_df.dropna(subset=[c], inplace=True)\n",
    "\n",
    "    movie_df = movie_df.reset_index(drop=True)\n",
    "\n",
    "# Helper function to remove movies that are not in the top 5 genres\n",
    "def topGenres(movie_df):\n",
    "    genres_set = set() \n",
    "    for i in movie_df['genres'].iteritems():\n",
    "        g = i[1].split('|')\n",
    "        genres_set.update(g)\n",
    "\n",
    "    genres = list(genres_set)\n",
    "    genres = dict.fromkeys(genres)\n",
    "    genres_ct = {key: 0 for key in genres} \n",
    "\n",
    "    # Count number of movies that belong to each genre\n",
    "    for i in movie_df['genres'].iteritems():\n",
    "        g = i[1].split('|')\n",
    "        for item in g: \n",
    "            if item in genres: \n",
    "                genres_ct[item] += 1 \n",
    "\n",
    "    # Get the top 5 genres \n",
    "    sortedCounts = [(genres_ct[key], key) for key in genres_ct]\n",
    "    sortedCounts.sort()\n",
    "    sortedCounts.reverse() \n",
    "    top_genres = [item[1] for item in sortedCounts[:5]] \n",
    "\n",
    "    # Build list of row indices that need to dropped because they are not in the top 5 genres\n",
    "    topg_set = set(top_genres) \n",
    "    drop = [] \n",
    "    for i in movie_df['genres'].iteritems():\n",
    "        g = set(i[1].split('|'))\n",
    "        if len(set.intersection(g, topg_set)) == 0:\n",
    "            drop.append(i)\n",
    "    \n",
    "movie_df_2015 = pd.read_csv('movie_scrape_2015.csv')\n",
    "deleteEmptyCol(movie_df_2015)\n",
    "del movie_df_2015['facenumber_in_poster']\n",
    "\n",
    "movie_df_2014 = pd.read_csv('movie_scrape_2014.csv')\n",
    "deleteEmptyCol(movie_df_2014)\n",
    "del movie_df_2014['facenumber_in_poster']\n",
    "\n",
    "movie_df_2013 = pd.read_csv('movie_scrape_2013.csv')\n",
    "deleteEmptyCol(movie_df_2013)\n",
    "del movie_df_2013['facenumber_in_poster']\n",
    "\n",
    "movie_df_2012 = pd.read_csv('movie_scrape_2012.csv')\n",
    "deleteEmptyCol(movie_df_2012)\n",
    "del movie_df_2012['facenumber_in_poster']\n",
    "\n",
    "movie_df_2011 = pd.read_csv('movie_scrape_2011.csv')\n",
    "deleteEmptyCol(movie_df_2011)\n",
    "\n",
    "movie_df_2010 = pd.read_csv('movie_scrape_2010.csv')\n",
    "deleteEmptyCol(movie_df_2010)\n",
    "\n",
    "movie_df_2009 = pd.read_csv('movie_scrape_2009.csv')\n",
    "deleteEmptyCol(movie_df_2009)\n",
    "\n",
    "movie_df_2008 = pd.read_csv('movie_scrape_2008.csv')\n",
    "deleteEmptyCol(movie_df_2008)\n",
    "\n",
    "movie_df_2007 = pd.read_csv('movie_scrape_2007.csv')\n",
    "deleteEmptyCol(movie_df_2007)\n",
    "\n",
    "movie_df_2006 = pd.read_csv('movie_scrape_2006.csv')\n",
    "deleteEmptyCol(movie_df_2006)\n",
    "\n",
    "movie_df_2005 = pd.read_csv('movie_scrape_2005.csv')\n",
    "deleteEmptyCol(movie_df_2005)\n",
    "\n",
    "movie_df_2004 = pd.read_csv('movie_scrape_2004.csv')\n",
    "deleteEmptyCol(movie_df_2004)\n",
    "\n",
    "kaggle_movie_df = pd.read_csv('movie_clean_metadata.csv')\n",
    "del kaggle_movie_df['facenumber_in_poster']\n",
    "\n",
    "frames = [movie_df_2015, movie_df_2014, movie_df_2013, movie_df_2012, movie_df_2011, movie_df_2010, movie_df_2009, movie_df_2008,\n",
    "          movie_df_2007, movie_df_2006, movie_df_2005, movie_df_2004, kaggle_movie_df]\n",
    "scraped_merge = pd.concat(frames)\n",
    "\n",
    "del scraped_merge['Unnamed: 0']\n",
    "\n",
    "topGenres(scraped_merge)\n",
    "scraped_merge = scraped_merge.drop_duplicates(subset=['movie_title'], keep=False)\n",
    "scraped_merge = scraped_merge.reset_index(drop=True)\n",
    "print(len(scraped_merge))\n",
    "\n",
    "scraped_merge.to_csv('movie_master_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
