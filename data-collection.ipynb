{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This script is responsible for intial results of \n",
    "cleaning CSV movie_metadata retrieved from kaggle.com\n",
    "'''\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "movie_df = pd.read_csv('movie_metadata.csv')\n",
    "\n",
    "# Deleting columns that are not useful \n",
    "delete = ['color', 'num_critic_for_reviews', 'actor_3_facebook_likes', 'actor_1_facebook_likes','movie_imdb_link', 'country', 'language', \n",
    "         'aspect_ratio', 'num_user_for_reviews', 'num_voted_users', 'actor_2_facebook_likes', 'content_rating', 'director_facebook_likes',\n",
    "         'cast_total_facebook_likes', 'movie_facebook_likes']\n",
    "for d in delete: del movie_df[d]\n",
    "\n",
    "# Delete rows with empty cells for any column     \n",
    "for c in movie_df: \n",
    "    movie_df[c].replace('', np.nan, inplace=True)\n",
    "    movie_df.dropna(subset=[c], inplace=True)\n",
    "\n",
    "movie_df = movie_df.reset_index(drop=True)\n",
    "\n",
    "genres_set = set() \n",
    "for i in range(len(movie_df)): \n",
    "    g = movie_df['genres'][i].split('|')\n",
    "    genres_set.update(g)\n",
    "\n",
    "genres = list(genres_set)\n",
    "genres = dict.fromkeys(genres)\n",
    "genres_ct = {key: 0 for key in genres} \n",
    "\n",
    "# Count number of movies that belong to each genre\n",
    "for i in range(len(movie_df)): \n",
    "    g = movie_df['genres'][i].split('|')\n",
    "    for item in g: \n",
    "        if item in genres: \n",
    "            genres_ct[item] += 1 \n",
    "\n",
    "# Get the top 5 genres \n",
    "sortedCounts = [(genres_ct[key], key) for key in genres_ct]\n",
    "sortedCounts.sort()\n",
    "sortedCounts.reverse() \n",
    "top_genres = [item[1] for item in sortedCounts[:5]] \n",
    "\n",
    "# Build list of row indices that need to dropped because they are not in the top 5 genres\n",
    "topg_set = set(top_genres) \n",
    "drop = [] \n",
    "for i in range(len(movie_df)):\n",
    "    g = set(movie_df['genres'][i].split('|'))\n",
    "    if len(set.intersection(g, topg_set)) == 0:\n",
    "        drop.append(i)\n",
    "\n",
    "movie_df = movie_df.drop(movie_df.index[drop])\n",
    "movie_df = movie_df.reset_index(drop=True) \n",
    "movie_df.to_csv('movie_clean_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-29298aa1683e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_base\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnext_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrapeData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mstop_ct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Retrieving next page of movies'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-29298aa1683e>\u001b[0m in \u001b[0;36mscrapeData\u001b[0;34m(soup)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mcsv_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_entry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlink_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "This script is responsible for scraping \n",
    "additional movie data from imdb.com\n",
    "'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import csv\n",
    "import re\n",
    "import datetime\n",
    "import cognitive_face as CF\n",
    "import time\n",
    "\n",
    "# Create or overwrite 'movie_scrape.csv' file\n",
    "columns = ['director_name', 'duration', 'actor_2_name', 'gross', 'genres', 'actor_1_name', 'movie_title', 'actor_3_name', 'facenumber_in_poster', 'plot_keywords', 'budget', 'title_year', 'imdb_score']\n",
    "csv_file = open('movie_scrape.csv', 'w')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(columns)\n",
    "\n",
    "# Helper function to remove all non-numeric characters from a string\n",
    "def removeNonNumeric(str):\n",
    "    return re.sub(\"[^0-9]\", \"\", str)\n",
    "\n",
    "# Function to scrape data from IMDB box office results\n",
    "def scrapeData(soup):\n",
    "    \n",
    "    # Finds the link for next page\n",
    "    pagination = soup.find('div', {'class':'desc'})\n",
    "    link_tag = pagination.find('a', {'class':'lister-page-next next-page'})\n",
    "    \n",
    "    for title in soup.find_all('h3', class_='lister-item-header'):\n",
    "        \n",
    "        movie_title = title.find('a').text\n",
    "        movie_url = title.find('a')['href']\n",
    "        m = urllib.request.urlopen('https://www.imdb.com/' + movie_url).read()\n",
    "        msoup = BeautifulSoup(m, 'html.parser')\n",
    "        \n",
    "        imdb_score = msoup.find_all('span', attrs={'itemprop':'ratingValue'})[0].text\n",
    "            \n",
    "        genre_div = msoup.find_all('div', itemprop='genre')[0].find_all('a')\n",
    "        i, genres = 0, ''\n",
    "        for g in genre_div:\n",
    "            i += 1\n",
    "            if i == len(genre_div):\n",
    "                genres += g.text\n",
    "            else:\n",
    "                genres += g.text + ' |'\n",
    "\n",
    "        director_name = msoup.find_all(attrs={'itemprop':'director'})[0].find('span').text\n",
    "        stars_spans = msoup.find_all(attrs={'itemprop':'actors'})\n",
    "        i, actor_1_name, actor_2_name, actor_3_name = 1, '', '', ''\n",
    "        for s in stars_spans:\n",
    "            if i == 1:\n",
    "                actor_1_name += s.find('span').text\n",
    "            elif i == 2:\n",
    "                actor_2_name += s.find('span').text\n",
    "            else:\n",
    "                actor_3_name += s.find('span').text\n",
    "            i += 1\n",
    "        \n",
    "        details_divs = msoup.find_all('div', id='titleDetails')[0].find_all('div')\n",
    "        budget, gross, title_year, duration = '', '', '', ''\n",
    "        for d in details_divs:\n",
    "            try:\n",
    "                if d.find('h4').text == 'Budget:':\n",
    "                    budget = removeNonNumeric(d.text.replace('\\n',''))\n",
    "                elif d.find('h4').text == 'Gross:':\n",
    "                    gross = removeNonNumeric(d.text.replace('\\n',''))\n",
    "                elif d.find('h4').text == 'Release Date:':\n",
    "                    date = d.text.split('\\n')[1].split('\\s')[0]\n",
    "                    title_year = re.search('\\d{4}', date).group(0)\n",
    "                elif d.find('h4').text == 'Runtime:':\n",
    "                    duration = removeNonNumeric(d.text.replace('\\n',''))\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "        keywords_span = msoup.find_all('span', attrs={'itemprop':'keywords'})\n",
    "        keywords = [key.text for key in keywords_span]\n",
    "        i, plot_keywords = 0, ''\n",
    "        for word in keywords: \n",
    "            i += 1 \n",
    "            if i == len(keywords):\n",
    "                plot_keywords += word \n",
    "            else: \n",
    "                plot_keywords += word + '|'\n",
    "        \n",
    "        poster_img_url = msoup.find_all('img', attrs={'itemprop':'image'})[0]['src']\n",
    "        KEY = '44053727cda344da8fbf58088dce0044'\n",
    "        CF.Key.set(KEY)\n",
    "        result = CF.face.detect(poster_img_url)\n",
    "        facenumber_in_poster = len(result)\n",
    "        \n",
    "        new_entry = [director_name, duration, actor_2_name, gross, genres, actor_1_name, movie_title, actor_3_name, facenumber_in_poster, plot_keywords, budget, title_year, imdb_score]\n",
    "        csv_writer.writerow(new_entry)\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "    return link_tag\n",
    "\n",
    "url_base = 'http://www.imdb.com/search/title' \n",
    "next_page = '?year=2015,2015&title_type=feature&sort=boxoffice_gross_us,desc'\n",
    "stop_ct = 0\n",
    "while True: \n",
    "    r = urllib.request.urlopen(url_base + next_page).read()\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "    tag = scrapeData(soup)\n",
    "    stop_ct += 1\n",
    "    print('Retrieving next page of movies')\n",
    "    if tag is None and i <= 20: \n",
    "        print('Done')\n",
    "        break\n",
    "    else:\n",
    "        nextPage = tag.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
